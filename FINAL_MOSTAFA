{"cells":[{"cell_type":"markdown","metadata":{"id":"V48sHZ3Q92nU"},"source":["# **<font color='red'>Final Project of The Course: Machine Learning</font>**\n","\n","<font color='purple'><strong>Mostafa Kermani Nia</strong></font> : 810101575  \n","<font color='purple'><strong>Aidin Kazemi</strong></font> : 810101561  \n","<font color='purple'><strong>Amir Naddaf Fahmideh</strong></font> : 810101540  \n","<font color='purple'><strong>Taha Majlesi</strong></font> : 810101504  \n"]},{"cell_type":"markdown","metadata":{"id":"sWGpk5bu9sGS"},"source":["# **<font color='orange'>PART 4: Classification</font>**"]},{"cell_type":"markdown","source":["# <font color='cyan'>Identity recognition</font>"],"metadata":{"id":"9eX6RC9x6E-j"}},{"cell_type":"markdown","source":["## <font color='Green'>Method 1 : **MLP**</font>"],"metadata":{"id":"wlgrX74c6V3k"}},{"cell_type":"markdown","source":["### **Import Libraries and Load the Dataset**\n","\n"],"metadata":{"id":"nlJi_q-l7qJQ"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n","from sklearn.neural_network import MLPClassifier\n","\n","# Load the dataset from CSV\n","data = pd.read_csv('voice_data.csv')\n","print(\"Dataset head:\")\n","print(data.head())"],"metadata":{"id":"_kKMcfeB65ic"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Libraries:** We import essential libraries such as `pandas` for data handling, `numpy` for numerical operations, and `matplotlib`/`seaborn` for plotting.\n","- **scikit-learn Modules:** The code uses `train_test_split` to partition the data, `StandardScaler` for normalization, and `MLPClassifier` for building our neural network model.\n","- **Data Loading:** The dataset is loaded from a CSV file named `'voice_data.csv'`. The first few rows are printed to ensure the data is loaded correctly.\n"],"metadata":{"id":"3gUC_k-369bV"}},{"cell_type":"markdown","source":["### **Preprocess Data (Features and Target Extraction & Encoding)**"],"metadata":{"id":"WpBgNV3e7_QL"}},{"cell_type":"code","source":["# Separate features and target variable\n","X = data.drop('gender', axis=1)\n","y = data['gender']\n","\n","# Encode target labels\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","print(\"\\nClasses after encoding:\", le.classes_)"],"metadata":{"id":"QSD8Wk6465c8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Feature-Target Separation:** The features (`X`) are all columns except `'gender'`, while the target (`y`) is the `'gender'` column.\n","- **Encoding:** If the target labels are in string format (like \"Male\" and \"Female\"), they are converted into numeric labels using `LabelEncoder`. This is essential for many machine learning models which require numeric input.\n"],"metadata":{"id":"Ec_GF2et7Gua"}},{"cell_type":"markdown","source":["### **Split the Dataset into Training and Test Sets**"],"metadata":{"id":"K7zt73MI8E6h"}},{"cell_type":"code","source":["# Split the data with stratification to maintain class proportions\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.25, stratify=y, random_state=42\n",")\n","print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n","print(f\"Test set size: {X_test.shape[0]} samples\")"],"metadata":{"id":"S2ANzMcg65aR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **Data Splitting:** The dataset is divided into training (75%) and test (25%) sets using `train_test_split`.\n","- **Stratification:** By setting `stratify=y`, we maintain the same proportion of classes in both training and test sets, which is vital for balanced evaluation.\n","- **Random State:** A fixed `random_state` ensures reproducibility.\n","\n"],"metadata":{"id":"MIo8p2ja7LfK"}},{"cell_type":"markdown","source":["### **Normalize the Features**"],"metadata":{"id":"TNmhoCl78IEg"}},{"cell_type":"code","source":["# Normalize features using StandardScaler\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"G3ezIJ6a65X0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **Normalization Purpose:** Many machine learning algorithms, including neural networks, perform better when input features are scaled. Here, normalization ensures that all features contribute equally to the model.\n","- **StandardScaler:** The scaler is fitted on the training data and then applied to both the training and test sets to prevent data leakage.\n","\n"],"metadata":{"id":"eAJUoCem7Pgk"}},{"cell_type":"markdown","source":["### **Build and Train the MLP Classifier**"],"metadata":{"id":"ffFNWhXJ8Kxd"}},{"cell_type":"code","source":["# Initialize and train the MLP classifier\n","mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n","mlp.fit(X_train, y_train)"],"metadata":{"id":"FjnbU-mC7PBX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **MLPClassifier Setup:** The Multi-Layer Perceptron is set up with one hidden layer containing 100 neurons. You can adjust the architecture as needed.\n","- **Training:** The model is trained on the normalized training data. The `max_iter` parameter limits the number of training iterations to prevent excessively long training times.\n","- **Random State:** Again, a fixed random state ensures that the results are reproducible.\n"],"metadata":{"id":"oqBLDZ5G7Uds"}},{"cell_type":"markdown","source":["### **Evaluate the Model**"],"metadata":{"id":"VaZIOOFa9JgD"}},{"cell_type":"code","source":["# Predict test set labels and probabilities for ROC analysis\n","y_pred = mlp.predict(X_test)\n","y_proba = mlp.predict_proba(X_test)[:, 1]\n","\n","# Compute and display the confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(\"\\nConfusion Matrix:\")\n","print(cm)\n","\n","plt.figure(figsize=(6,4))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","plt.title(\"Confusion Matrix - MLP Classifier\")\n","plt.xlabel(\"Predicted Labels\")\n","plt.ylabel(\"True Labels\")\n","plt.show()\n","\n","# Generate and print the classification report\n","report = classification_report(y_test, y_pred, target_names=le.classes_)\n","print(\"\\nClassification Report:\")\n","print(report)"],"metadata":{"id":"tUhmz7az7O-2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **Predictions:** The model predicts class labels for the test set. Additionally, predicted probabilities are obtained for ROC curve analysis.\n","- **Confusion Matrix:** This matrix shows the counts of true positives, true negatives, false positives, and false negatives. It provides a quick overview of how well the classifier performed.\n","- **Visualization:** A heatmap is used to graphically display the confusion matrix, making it easier to interpret.\n","- **Classification Report:** This report includes precision, recall, f1-score, and support for each class, giving detailed insights into the model’s performance.\n"],"metadata":{"id":"QP9cqWnn7Xzo"}},{"cell_type":"markdown","source":["### **Plot the ROC Curve and Compute AUC**"],"metadata":{"id":"U3zjai2k9Mro"}},{"cell_type":"code","source":["# Compute ROC curve and AUC\n","fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n","roc_auc = auc(fpr, tpr)\n","\n","plt.figure(figsize=(6,4))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) - MLP Classifier')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"metadata":{"id":"C0SMYQhw7O8S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **ROC Curve:** The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at different classification thresholds. It is an effective tool for visualizing the trade-off between sensitivity and specificity.\n","- **AUC:** The Area Under the Curve (AUC) summarizes the ROC curve into a single number. A higher AUC indicates better overall model performance.\n","- **Visualization:** The ROC curve is plotted with a reference diagonal line (representing random chance) for comparison.\n"],"metadata":{"id":"i64M-bGa7cZA"}},{"cell_type":"markdown","source":["### **Error Analysis**"],"metadata":{"id":"crZfqJJJ9Qyp"}},{"cell_type":"code","source":["tn, fp, fn, tp = cm.ravel()\n","fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n","fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n","\n","print(\"\\nError Analysis:\")\n","print(f\"False Negative Rate (Type II error): {fn_rate:.2f}\")\n","print(f\"False Positive Rate (Type I error): {fp_rate:.2f}\")\n"],"metadata":{"id":"rFkclvL97b9V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **Error Metrics:**  \n","  - **False Negative Rate (FNR):** This indicates the proportion of positive instances that were incorrectly classified as negative (i.e., missed detections).\n","  - **False Positive Rate (FPR):** This indicates the proportion of negative instances that were incorrectly classified as positive.\n","- **Purpose:** These metrics help in understanding the nature of the errors made by the classifier, which is crucial for domains where one type of error might be more critical than the other.\n","\n"],"metadata":{"id":"wAloH4-q7f90"}},{"cell_type":"markdown","source":["### **Summary**"],"metadata":{"id":"t8mhJ1BT9Ud_"}},{"cell_type":"markdown","source":["In this implementation, we have:\n","1. **Loaded and explored the dataset** to ensure proper structure.\n","2. **Separated features and target, and encoded the target labels** for model compatibility.\n","3. **Split the dataset into training and test sets** with stratification to preserve the class distribution.\n","4. **Normalized the data** to improve model performance.\n","5. **Built and trained an MLP classifier** with one hidden layer.\n","6. **Evaluated the model** using a confusion matrix, classification report, and ROC curve with AUC.\n","7. **Performed error analysis** to identify the rates of false positives and false negatives.\n","\n","This thorough step-by-step approach not only builds the classifier but also provides extensive visualization and metrics to understand the model's performance. Feel free to run each section individually to observe the outputs and graphs, and let me know if you have any questions or need further modifications!"],"metadata":{"id":"rfCt6LLb7jLE"}},{"cell_type":"markdown","source":["## <font color='Green'>Method 2 : **KNN**</font>"],"metadata":{"id":"7jBTT-vM6e1o"}},{"cell_type":"markdown","source":["### **Import Libraries and Load the Dataset**"],"metadata":{"id":"m6KBgOq1-aM8"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# Load the dataset from CSV\n","data = pd.read_csv('voice_data.csv')\n","print(\"Dataset head:\")\n","print(data.head())"],"metadata":{"id":"RNyuvaet-YyN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **pandas & numpy:**  \n","  Used for data manipulation and numerical operations.\n","  \n","- **matplotlib & seaborn:**  \n","  Essential for creating visualizations like plots and heatmaps.\n","  \n","- **train_test_split (sklearn):**  \n","  Splits the dataset into training and testing sets.\n","  \n","- **StandardScaler & LabelEncoder (sklearn):**  \n","  - **StandardScaler:** Normalizes features (zero mean, unit variance).  \n","  - **LabelEncoder:** Converts categorical labels (e.g., \"Male\", \"Female\") into numeric values.\n","  \n","- **confusion_matrix, classification_report, roc_curve, auc (sklearn):**  \n","  Provide evaluation metrics and performance analysis.\n","  \n","- **KNeighborsClassifier (sklearn):**  \n","  Implements the K-Nearest Neighbors algorithm for classification.\n","  \n","- **Data Loading:**  \n","  The CSV file `'voice_data.csv'` is read into a DataFrame, and the first few rows are printed for a quick overview of the dataset.\n"],"metadata":{"id":"IBhTVrf89qyH"}},{"cell_type":"markdown","source":["### **Preprocess the Data**"],"metadata":{"id":"pp892jb1-evP"}},{"cell_type":"code","source":["# Separate features and target variable\n","X = data.drop('gender', axis=1)\n","y = data['gender']\n","\n","# Encode target labels\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","print(\"\\nClasses after encoding:\", le.classes_)\n"],"metadata":{"id":"XEhRNetA9s3l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **Feature & Target Separation:**  \n","  - **`data.drop('gender', axis=1)`:** Removes the target column ('gender') from the features.\n","  - **`data['gender']`:** Extracts the target column.\n","  \n","- **Label Encoding:**  \n","  - **`LabelEncoder()`:** Converts categorical labels into numeric values, enabling the model to process them.\n","  - **`fit_transform()`:** Simultaneously fits the encoder to the target labels and transforms them.\n"],"metadata":{"id":"L6IViXUU93Oy"}},{"cell_type":"markdown","source":["### **Split the Dataset**"],"metadata":{"id":"mmLXTY43-g4k"}},{"cell_type":"code","source":["# Split the data with stratification to maintain class proportions\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.25, stratify=y, random_state=42\n",")\n","print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n","print(f\"Test set size: {X_test.shape[0]} samples\")"],"metadata":{"id":"ilvzEpJF9s1K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **train_test_split:**\n","  - **`test_size=0.25`:** Allocates 25% of the data for testing.\n","  - **`stratify=y`:** Preserves the class distribution in both training and testing sets.\n","  - **`random_state=42`:** Ensures reproducibility of the split.\n"],"metadata":{"id":"oiRM_frw96wB"}},{"cell_type":"markdown","source":["### **Normalize the Features**"],"metadata":{"id":"BJXLazo4-lgn"}},{"cell_type":"code","source":["# Normalize features using StandardScaler\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"Qd8h9obG9szA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **StandardScaler:**\n","  - **`fit_transform()`:** Computes the mean and standard deviation on the training set and scales the data.\n","  - **`transform()`:** Applies the same scaling to the test set to ensure consistency.\n","  \n","- **Normalization:**  \n","  This step is vital, especially for KNN, because distance-based algorithms are sensitive to the scale of the features.\n"],"metadata":{"id":"4yJG7rQt-AEj"}},{"cell_type":"markdown","source":["### **Build and Train the KNN Classifier**"],"metadata":{"id":"0_ZC9OcO-oPp"}},{"cell_type":"code","source":["# Initialize and train the KNN classifier\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)"],"metadata":{"id":"-9Tfl1Ly9swk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **KNeighborsClassifier:**\n","  - **`n_neighbors=5`:** Sets the number of neighbors to consider when classifying a new sample. You can adjust this parameter as needed.\n","  \n","- **Model Training:**\n","  - **`fit()`:** Trains the KNN model using the normalized training data (`X_train`) and corresponding labels (`y_train`).\n"],"metadata":{"id":"XTH9SGhs-Edf"}},{"cell_type":"markdown","source":["### **Evaluate the Model**"],"metadata":{"id":"kID71i-F-rir"}},{"cell_type":"code","source":["# Predict test set labels and probabilities for ROC analysis\n","y_pred = knn.predict(X_test)\n","y_proba = knn.predict_proba(X_test)[:, 1]\n","\n","# Compute and display the confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(\"\\nConfusion Matrix:\")\n","print(cm)\n","\n","plt.figure(figsize=(6,4))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","plt.title(\"Confusion Matrix - KNN Classifier\")\n","plt.xlabel(\"Predicted Labels\")\n","plt.ylabel(\"True Labels\")\n","plt.show()\n","\n","# Generate and print the classification report\n","report = classification_report(y_test, y_pred, target_names=le.classes_)\n","print(\"\\nClassification Report:\")\n","print(report)"],"metadata":{"id":"fxRCtYJY9suN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Prediction:**\n","  - **`predict()`:** Generates class label predictions for the test set.\n","  - **`predict_proba()`:** Provides probability estimates for each class; the probabilities for the positive class are used for ROC analysis.\n","  \n","- **Evaluation Metrics:**\n","  - **`confusion_matrix()`:** Compares true labels (`y_test`) with predictions (`y_pred`) to assess model performance.\n","  - **`classification_report()`:** Displays detailed metrics such as precision, recall, f1-score, and support for each class.\n","  \n","- **Visualization:**\n","  - **Seaborn's `heatmap()`:** Visualizes the confusion matrix for easier interpretation of classification results.\n"],"metadata":{"id":"-_khhWH4-HkL"}},{"cell_type":"markdown","source":["### **Plot the ROC Curve and Compute AUC**"],"metadata":{"id":"vWrTiMWH-u_R"}},{"cell_type":"code","source":["# Compute ROC curve and AUC\n","fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n","roc_auc = auc(fpr, tpr)\n","\n","plt.figure(figsize=(6,4))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) - KNN Classifier')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"metadata":{"id":"FEV3VVYJ9srp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **ROC Analysis:**\n","  - **`roc_curve()`:** Computes the false positive rate (FPR) and true positive rate (TPR) across different thresholds.\n","  - **`auc()`:** Calculates the Area Under the ROC Curve, summarizing the classifier's performance.\n","  \n","- **Visualization:**\n","  - **`plt.plot()`:** Plots the ROC curve.\n","  - **Reference Diagonal Line:** A dashed line from (0, 0) to (1, 1) represents a random classifier’s performance for comparison."],"metadata":{"id":"BMY_VPph-MVN"}},{"cell_type":"markdown","source":["### **Error Analysis**"],"metadata":{"id":"XO45P7_a-yzJ"}},{"cell_type":"code","source":["tn, fp, fn, tp = cm.ravel()\n","fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n","fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n","\n","print(\"\\nError Analysis:\")\n","print(f\"False Negative Rate (Type II error): {fn_rate:.2f}\")\n","print(f\"False Positive Rate (Type I error): {fp_rate:.2f}\")"],"metadata":{"id":"r1iEADoY9sCY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **Confusion Matrix Unpacking:**\n","  - **`ravel()`:** Flattens the 2x2 confusion matrix into individual values: True Negatives (TN), False Positives (FP), False Negatives (FN), and True Positives (TP).\n","  \n","- **Error Rates:**\n","  - **False Negative Rate (FNR):** Measures the proportion of positive instances that were misclassified as negative.\n","  - **False Positive Rate (FPR):** Measures the proportion of negative instances that were misclassified as positive.\n","  \n","- **Conditional Calculation:**  \n","  Checks to prevent division by zero when calculating error rates.\n","\n"],"metadata":{"id":"reVhSaYh-UkI"}},{"cell_type":"markdown","source":["### **Summary**"],"metadata":{"id":"6jSrWc4i_CCX"}},{"cell_type":"markdown","source":["\n","This implementation follows the same structure as before, adapted for the KNN classifier. It covers data loading, preprocessing, model training, evaluation (including confusion matrix, classification report, and ROC analysis), and error analysis. Run each section sequentially to observe the outputs and visualizations, and feel free to adjust parameters (such as the number of neighbors) as needed.\n"],"metadata":{"id":"fYQdM26U_F9o"}},{"cell_type":"markdown","source":["# <font color='Green'>Method 3 : **SVM**</font>"],"metadata":{"id":"G64TOQ2Z6e6X"}},{"cell_type":"markdown","source":["Below is the complete classification implementation using the Support Vector Machine (SVM) method. The code is divided into sections with minimal inline comments. Under each section, you'll find explanations highlighting the key points and important functions used.\n","\n","---\n","\n","## **Section 1: Import Libraries and Load the Dataset**\n","\n","```python\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n","from sklearn.svm import SVC\n","\n","# Load the dataset from CSV\n","data = pd.read_csv('voice_data.csv')\n","print(\"Dataset head:\")\n","print(data.head())\n","```\n","\n","**Key Points & Important Functions:**\n","\n","- **pandas & numpy:**  \n","  Used for data manipulation and numerical operations.\n","  \n","- **matplotlib & seaborn:**  \n","  Essential for creating visualizations such as plots and heatmaps.\n","  \n","- **train_test_split (sklearn):**  \n","  Splits the data into training and testing sets.\n","  \n","- **StandardScaler & LabelEncoder (sklearn):**  \n","  - **StandardScaler:** Normalizes features (zero mean, unit variance).  \n","  - **LabelEncoder:** Converts categorical labels (e.g., \"Male\", \"Female\") into numeric values.\n","  \n","- **confusion_matrix, classification_report, roc_curve, auc (sklearn):**  \n","  Provide evaluation metrics and performance analysis.\n","  \n","- **SVC (sklearn):**  \n","  Implements the Support Vector Machine classifier. Here, we enable probability estimates (with `probability=True`) for ROC analysis.\n","  \n","- **Data Loading:**  \n","  The CSV file `'voice_data.csv'` is read into a DataFrame, and the first few rows are printed to verify the dataset structure.\n","\n","---\n","\n","## **Section 2: Preprocess the Data**\n","\n","```python\n","# Separate features and target variable\n","X = data.drop('gender', axis=1)\n","y = data['gender']\n","\n","# Encode target labels\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","print(\"\\nClasses after encoding:\", le.classes_)\n","```\n","\n","**Key Points & Important Functions:**\n","\n","- **Feature & Target Separation:**  \n","  - **`data.drop('gender', axis=1)`:** Removes the target column ('gender') from the features.\n","  - **`data['gender']`:** Extracts the target column.\n","  \n","- **Label Encoding:**  \n","  - **`LabelEncoder()`:** Converts categorical labels into numeric values, allowing the model to process them.\n","  - **`fit_transform()`:** Fits the encoder on the target labels and transforms them in one step.\n","\n","---\n","\n","## **Section 3: Split the Dataset**\n","\n","```python\n","# Split the data with stratification to maintain class proportions\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.25, stratify=y, random_state=42\n",")\n","print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n","print(f\"Test set size: {X_test.shape[0]} samples\")\n","```\n","\n","**Key Points & Important Functions:**\n","\n","- **train_test_split:**\n","  - **`test_size=0.25`:** Allocates 25% of the data for testing.\n","  - **`stratify=y`:** Preserves the class distribution in both the training and testing sets.\n","  - **`random_state=42`:** Ensures reproducibility of the data split.\n","\n","---\n","\n","## **Section 4: Normalize the Features**\n","\n","```python\n","# Normalize features using StandardScaler\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","```\n","\n","**Key Points & Important Functions:**\n","\n","- **StandardScaler:**\n","  - **`fit_transform()`:** Computes the mean and standard deviation on the training set and scales the data accordingly.\n","  - **`transform()`:** Applies the same scaling parameters to the test set.\n","  \n","- **Normalization:**  \n","  Vital for SVM because support vector machines are sensitive to the scale of the features.\n","\n","---\n","\n","## **Section 5: Build and Train the SVM Classifier**\n","\n","```python\n","# Initialize and train the SVM classifier with probability estimates enabled\n","svm = SVC(kernel='rbf', probability=True, random_state=42)\n","svm.fit(X_train, y_train)\n","```\n","\n","**Key Points & Important Functions:**\n","\n","- **SVC (Support Vector Classifier):**\n","  - **`kernel='rbf'`:** Uses the Radial Basis Function (RBF) kernel, which is common for non-linear classification tasks.\n","  - **`probability=True`:** Enables probability estimates needed for ROC analysis.\n","  - **`random_state=42`:** Ensures reproducible results.\n","  \n","- **Model Training:**\n","  - **`fit()`:** Trains the SVM model using the normalized training data (`X_train`) and corresponding labels (`y_train`).\n","\n","---\n","\n","## **Section 6: Evaluate the Model**\n","\n","```python\n","# Predict test set labels and probabilities for ROC analysis\n","y_pred = svm.predict(X_test)\n","y_proba = svm.predict_proba(X_test)[:, 1]\n","\n","# Compute and display the confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(\"\\nConfusion Matrix:\")\n","print(cm)\n","\n","plt.figure(figsize=(6,4))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","plt.title(\"Confusion Matrix - SVM Classifier\")\n","plt.xlabel(\"Predicted Labels\")\n","plt.ylabel(\"True Labels\")\n","plt.show()\n","\n","# Generate and print the classification report\n","report = classification_report(y_test, y_pred, target_names=le.classes_)\n","print(\"\\nClassification Report:\")\n","print(report)\n","```\n","\n","**Key Points & Important Functions:**\n","\n","- **Prediction:**\n","  - **`predict()`:** Generates class label predictions for the test set.\n","  - **`predict_proba()`:** Provides probability estimates for each class; the probabilities for the positive class are used for ROC analysis.\n","  \n","- **Evaluation Metrics:**\n","  - **`confusion_matrix()`:** Compares the true labels (`y_test`) with predictions (`y_pred`) to evaluate model performance.\n","  - **`classification_report()`:** Summarizes key metrics like precision, recall, f1-score, and support for each class.\n","  \n","- **Visualization:**\n","  - **Seaborn's `heatmap()`:** Visualizes the confusion matrix, aiding in the interpretation of classification results.\n","\n","---\n","\n","## **Section 7: Plot the ROC Curve and Compute AUC**\n","\n","```python\n","# Compute ROC curve and AUC\n","fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n","roc_auc = auc(fpr, tpr)\n","\n","plt.figure(figsize=(6,4))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) - SVM Classifier')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","```\n","\n","**Key Points & Important Functions:**\n","\n","- **ROC Analysis:**\n","  - **`roc_curve()`:** Computes the false positive rate (FPR) and true positive rate (TPR) across different thresholds, facilitating the evaluation of trade-offs between sensitivity and specificity.\n","  - **`auc()`:** Calculates the Area Under the ROC Curve, providing a single-value summary of the classifier’s performance.\n","  \n","- **Visualization:**\n","  - **`plt.plot()`:** Plots the ROC curve.\n","  - **Reference Diagonal Line:** The dashed line from (0,0) to (1,1) serves as a benchmark representing a random classifier.\n","\n","---\n","\n","## **Section 8: Error Analysis**\n","\n","```python\n","tn, fp, fn, tp = cm.ravel()\n","fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n","fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n","\n","print(\"\\nError Analysis:\")\n","print(f\"False Negative Rate (Type II error): {fn_rate:.2f}\")\n","print(f\"False Positive Rate (Type I error): {fp_rate:.2f}\")\n","```\n","\n","**Key Points & Important Functions:**\n","\n","- **Confusion Matrix Unpacking:**\n","  - **`ravel()`:** Flattens the 2x2 confusion matrix into individual values: True Negatives (TN), False Positives (FP), False Negatives (FN), and True Positives (TP).\n","  \n","- **Error Rates:**\n","  - **False Negative Rate (FNR):** Measures the proportion of positive cases that were missed (misclassified as negative).\n","  - **False Positive Rate (FPR):** Measures the proportion of negative cases that were incorrectly classified as positive.\n","  \n","- **Conditional Calculation:**  \n","  Checks are included to prevent division by zero during the error rate calculations.\n","\n","---\n","\n","This implementation follows the same structure as the KNN example, but it uses the SVM classifier with an RBF kernel. It covers data loading, preprocessing, model training, evaluation (including confusion matrix, classification report, and ROC analysis), and error analysis. Run each section sequentially to observe the outputs and visualizations, and adjust any parameters as needed for your specific dataset and requirements."],"metadata":{"id":"WYd7ievRAkB3"}},{"cell_type":"markdown","source":["# <font color='Green'>**Compare 3 methods**</font>"],"metadata":{"id":"TDAyvjCw6vdL"}},{"cell_type":"markdown","source":["Below is a complete Python implementation that compares the three classification methods—MLP, KNN, and SVM—using the same dataset and preprocessing pipeline. The code is divided into sections with minimal inline comments. After each section, you’ll find key points and explanations that highlight the important functions and steps used to compare these methods.\n","\n","---\n","\n","## **Section 1: Import Libraries, Load, and Preprocess the Dataset**\n","\n","```python\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","\n","# Load the dataset from CSV\n","data = pd.read_csv('voice_data.csv')\n","print(\"Dataset Head:\")\n","print(data.head())\n","\n","# Separate features and target variable\n","X = data.drop('gender', axis=1)\n","y = data['gender']\n","\n","# Encode target labels\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","print(\"\\nClasses after encoding:\", le.classes_)\n","\n","# Split the data (25% test) with stratification to maintain class proportions\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.25, stratify=y, random_state=42\n",")\n","\n","# Normalize features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","```\n","\n","**Key Points & Explanations:**\n","\n","- **Data Loading:**  \n","  The CSV file (`voice_data.csv`) is read into a DataFrame, and the first few rows are printed to verify the dataset’s structure.\n","\n","- **Preprocessing:**  \n","  - The target column `'gender'` is separated from the features.\n","  - **LabelEncoder** converts categorical labels (e.g., \"Male\", \"Female\") into numeric values.\n","  - **train_test_split** divides the data into training (75%) and test (25%) sets while preserving the class distribution via the `stratify` parameter.\n","  - **StandardScaler** normalizes the features, which is especially important for algorithms like SVM and KNN that are sensitive to feature scales.\n","\n","---\n","\n","## **Section 2: Initialize and Train the Classifiers**\n","\n","```python\n","# Initialize classifiers with chosen parameters\n","mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n","knn = KNeighborsClassifier(n_neighbors=5)\n","svm = SVC(kernel='rbf', probability=True, random_state=42)\n","\n","# Train the models on the training data\n","mlp.fit(X_train, y_train)\n","knn.fit(X_train, y_train)\n","svm.fit(X_train, y_train)\n","```\n","\n","**Key Points & Explanations:**\n","\n","- **MLPClassifier:**  \n","  Uses one hidden layer with 100 neurons and a maximum of 300 iterations.  \n","- **KNeighborsClassifier:**  \n","  Considers 5 nearest neighbors (this parameter can be tuned).  \n","- **SVC:**  \n","  Uses the RBF kernel for non-linear separation and enables probability estimates (with `probability=True`) for ROC analysis.\n","- **Model Training:**  \n","  All three models are trained using the same preprocessed training data to ensure a fair comparison.\n","\n","---\n","\n","## **Section 3: Evaluate Each Classifier and Collect Results**\n","\n","```python\n","# Create a dictionary to hold our models and their evaluation results\n","models = {'MLP': mlp, 'KNN': knn, 'SVM': svm}\n","results = {}\n","\n","# Evaluate each classifier\n","for name, model in models.items():\n","    y_pred = model.predict(X_test)\n","    y_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n","    cm = confusion_matrix(y_test, y_pred)\n","    clf_report = classification_report(y_test, y_pred, target_names=le.classes_)\n","    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n","    roc_auc = auc(fpr, tpr)\n","    \n","    # Store results for comparison\n","    results[name] = {\n","        'y_pred': y_pred,\n","        'y_proba': y_proba,\n","        'cm': cm,\n","        'report': clf_report,\n","        'fpr': fpr,\n","        'tpr': tpr,\n","        'roc_auc': roc_auc\n","    }\n","    \n","    # Print evaluation metrics\n","    print(f\"\\n{name} Classifier:\")\n","    print(\"Confusion Matrix:\")\n","    print(cm)\n","    print(\"Classification Report:\")\n","    print(clf_report)\n","    print(f\"ROC AUC: {roc_auc:.2f}\")\n","```\n","\n","**Key Points & Explanations:**\n","\n","- **Prediction & Probabilities:**  \n","  Each model predicts the labels for the test set and calculates class probabilities (needed for ROC analysis).\n","\n","- **Evaluation Metrics:**  \n","  - **confusion_matrix**: Provides counts of true positives, true negatives, false positives, and false negatives.\n","  - **classification_report**: Gives precision, recall, f1-score, and support for each class.\n","  - **roc_curve** and **auc**: Compute and summarize the Receiver Operating Characteristic (ROC) performance.\n","\n","- **Results Dictionary:**  \n","  All key outputs (predictions, confusion matrix, classification report, ROC data, and AUC) are stored in a dictionary for later comparison and visualization.\n","\n","---\n","\n","## **Section 4: Compare the ROC Curves**\n","\n","\n","\n","Below is the complete implementation that compares the three methods (MLP, KNN, and SVM) on the same dataset. The code is divided into sections with minimal inline comments. After each section, key points and explanations describe the important functions and steps used to perform the comparison.\n","\n","---\n","\n","## **Section 1: Import Libraries, Load, and Preprocess the Dataset**\n","\n","```python\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","\n","# Load the dataset from CSV\n","data = pd.read_csv('voice_data.csv')\n","print(\"Dataset Head:\")\n","print(data.head())\n","\n","# Separate features and target variable\n","X = data.drop('gender', axis=1)\n","y = data['gender']\n","\n","# Encode target labels\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","print(\"\\nClasses after encoding:\", le.classes_)\n","\n","# Split the data (25% test) with stratification to maintain class proportions\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.25, stratify=y, random_state=42\n",")\n","\n","# Normalize features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","```\n","\n","**Key Points & Explanations:**\n","\n","- **Data Loading & Exploration:**  \n","  - Uses **pandas** to load the CSV file and preview the dataset.\n","  \n","- **Preprocessing:**  \n","  - The target column `'gender'` is separated from the features.\n","  - **LabelEncoder** converts string labels (e.g., \"Male\", \"Female\") into numeric values.\n","  - **train_test_split** splits the data into training (75%) and test (25%) sets while preserving the class proportions with `stratify=y`.\n","  - **StandardScaler** normalizes the feature data, ensuring that all models work on data with the same scale.\n","\n","---\n","\n","## **Section 2: Initialize and Train the Classifiers**\n","\n","```python\n","# Initialize classifiers with chosen parameters\n","mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n","knn = KNeighborsClassifier(n_neighbors=5)\n","svm = SVC(kernel='rbf', probability=True, random_state=42)\n","\n","# Train the models on the training data\n","mlp.fit(X_train, y_train)\n","knn.fit(X_train, y_train)\n","svm.fit(X_train, y_train)\n","```\n","\n","**Key Points & Explanations:**\n","\n","- **Model Initialization:**  \n","  - **MLPClassifier:** Configured with one hidden layer (100 neurons) and 300 iterations.\n","  - **KNeighborsClassifier:** Uses 5 neighbors; this is the distance-based classifier.\n","  - **SVC:** Employs an RBF kernel and enables probability estimates (with `probability=True`) for later ROC analysis.\n","  \n","- **Model Training:**  \n","  - Each model is trained on the same normalized training data to ensure a fair comparison.\n","\n","---\n","\n","## **Section 3: Evaluate Each Classifier and Collect Results**\n","\n","```python\n","# Create a dictionary to hold our models and their evaluation results\n","models = {'MLP': mlp, 'KNN': knn, 'SVM': svm}\n","results = {}\n","\n","# Evaluate each classifier\n","for name, model in models.items():\n","    y_pred = model.predict(X_test)\n","    y_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n","    cm = confusion_matrix(y_test, y_pred)\n","    clf_report = classification_report(y_test, y_pred, target_names=le.classes_)\n","    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n","    roc_auc = auc(fpr, tpr)\n","    \n","    # Store results for later comparison\n","    results[name] = {\n","        'y_pred': y_pred,\n","        'y_proba': y_proba,\n","        'cm': cm,\n","        'report': clf_report,\n","        'fpr': fpr,\n","        'tpr': tpr,\n","        'roc_auc': roc_auc\n","    }\n","    \n","    # Print evaluation metrics for each classifier\n","    print(f\"\\n{name} Classifier:\")\n","    print(\"Confusion Matrix:\")\n","    print(cm)\n","    print(\"Classification Report:\")\n","    print(clf_report)\n","    print(f\"ROC AUC: {roc_auc:.2f}\")\n","```\n","\n","**Key Points & Explanations:**\n","\n","- **Evaluation Metrics:**  \n","  - **predict()** and **predict_proba()** generate predictions and probability estimates for the test set.\n","  - **confusion_matrix** provides a breakdown of true vs. false classifications.\n","  - **classification_report** gives precision, recall, f1-score, and support for each class.\n","  - **roc_curve** and **auc** compute the ROC data and summarize performance with the Area Under the Curve (AUC).\n","  \n","- **Results Storage:**  \n","  All outputs for each classifier are stored in the `results` dictionary, making it easier to compare later.\n","\n","---\n","\n","## **Section 4: Plot and Compare ROC Curves**\n","\n","```python\n","# Plot ROC curves for all classifiers in one plot for comparison\n","plt.figure(figsize=(8, 6))\n","colors = {'MLP': 'darkorange', 'KNN': 'green', 'SVM': 'blue'}\n","\n","for name, res in results.items():\n","    plt.plot(res['fpr'], res['tpr'], color=colors[name], lw=2, label=f'{name} (AUC = {res[\"roc_auc\"]:.2f})')\n","\n","# Plot the diagonal line representing a random classifier\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve Comparison: MLP vs. KNN vs. SVM')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","```\n","\n","**Key Points & Explanations:**\n","\n","- **Combined ROC Curve:**  \n","  - The ROC curves for MLP, KNN, and SVM are plotted on a single graph. This visual comparison allows you to quickly assess which model achieves a higher true positive rate at lower false positive rates.\n","  - The diagonal dashed line represents the performance of a random classifier.\n","  \n","- **AUC Comparison:**  \n","  - Each classifier’s ROC curve is labeled with its corresponding AUC value. A higher AUC generally indicates better overall performance.\n","\n","---\n","\n","## **Section 5: Overall Comparison and Discussion**\n","\n","```python\n","# Optionally, you can compare additional metrics or print a summary\n","print(\"\\nOverall Comparison:\")\n","for name, res in results.items():\n","    print(f\"{name} Classifier AUC: {res['roc_auc']:.2f}\")\n","    # Additional metrics like accuracy, precision, or recall can also be computed here if desired.\n","```\n","\n","**Key Points & Explanations:**\n","\n","- **Summary of Metrics:**  \n","  This section prints out the AUC for each classifier. You can extend this summary to include other metrics (e.g., accuracy, precision, recall) to further compare the models.\n","  \n","- **Interpretation:**  \n","  - The classifier with the highest AUC and favorable precision/recall values is typically preferred.\n","  - Depending on the application, you may also consider the confusion matrices and classification reports to understand the types of errors (false positives vs. false negatives) each model makes.\n","\n","---\n","\n","## **Final Discussion**\n","\n","- **MLP Classifier:**  \n","  May perform well on non-linear problems but requires careful tuning of hyperparameters (e.g., number of neurons, learning rate). It is sensitive to the scale of the features, which is why normalization is essential.\n","\n","- **KNN Classifier:**  \n","  A simple, instance-based method that can work very well when the decision boundaries are clear. Its performance is heavily dependent on the choice of `n_neighbors` and the scale of features.\n","\n","- **SVM Classifier:**  \n","  Effective in high-dimensional spaces and uses kernel functions (RBF in this case) to handle non-linear classification. Enabling probability estimates allows for ROC analysis, though training can be computationally intensive with large datasets.\n","\n","By comparing the ROC curves and AUC values along with the detailed classification reports and confusion matrices, you can decide which model best fits your application’s requirements and performance criteria.\n","\n","---\n","\n","This complete implementation and its accompanying explanations allow you to compare the performance of MLP, KNN, and SVM classifiers on your dataset. Adjust model parameters as needed to further refine each method’s performance."],"metadata":{"id":"nYpwkJ-FAah_"}},{"cell_type":"markdown","source":["### <font color='cyan'>Gender recognition</font>"],"metadata":{"id":"pij7HGEM6LDY"}},{"cell_type":"code","source":[],"metadata":{"id":"71w_lfpY092b"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}